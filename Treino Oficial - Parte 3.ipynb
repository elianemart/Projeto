{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "import gensim\n",
      "from collections import Counter\n",
      "import nltk\n",
      "from nltk.corpus import stopwords.words('english')\n",
      "from nltk.corpus import stopwords.words('portuguese')\n",
      "\n",
      "\n",
      "path_en = \"/home/lili/Documents/Projeto Final do Mestrado/treino/ingles/\"\n",
      "path_pt = \"/home/lili/Documents/Projeto Final do Mestrado/treino/portugues/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "A fun\u00e7\u00e3o word2vec vai rodar duas passagens no iterador de frases.\n",
      ".A primeira coleta as palavras e registra suas frequencias para construir uma estrutura de dicionario interna \n",
      ".A segunda treina a rede neural.\n",
      "Esses processos tamb\u00e9m podem ser gerados manualmente separadamente:\n",
      "model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
      "model.train(other_sentences)  # can be a non-repeatable, 1-pass generator"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Principais Parametros:\n",
      "\n",
      ".Frequ\u00eancia minima de uma palavra para ela n\u00e3o ser descartada do treino\n",
      ">>> model = Word2Vec(sentences, min_count=10)  # default value is 5, o razo\u00e1vel \u00e9 entre 0-100\n",
      "\n",
      "Outro paramentro \u00e9 o tamanho da camada oculta da rede neural, que ir\u00e1 corresponder aos graus de liberdade do algoritmo de treino. Valores maiores requerem mais dados de treino, mas podem levar a melhores e mais acurados modelos. Valores razoaveis s\u00e3o de 10-1000\n",
      ">>> model = Word2Vec(sentences, size=200)  # default value is 100\n",
      "\n",
      "Para treino paralelo, para acelerar o processo.\n",
      "S\u00f3 funciona com o Cython instalado.\n",
      ">>> model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization\n",
      "Try alpha=0.05 and cbow_mean=1."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Para o Portugu\u00eas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_in = path_pt + \"passo6/ptfile\"\n",
      "\n",
      "for j in range(26):\n",
      "    f = codecs.open(data_in + str(j) + '.txt', encoding='utf-8')\n",
      "    f.seek(0)\n",
      "    text = f.read().split()\n",
      "\n",
      "    frase = []\n",
      "    for i in range(len(text)):\n",
      "        if text[i] != u'.':\n",
      "            frase.append(text[i])\n",
      "        else:\n",
      "            sentences.append(frase)\n",
      "            frase = []\n",
      "\n",
      "    if len(frase) != 0:\n",
      "        sentences.append(frase)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, alpha=0.05, cbow_mean =1, workers=4)\n",
      "\n",
      "model.save(\"model-portuguese\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Para o Ingl\u00eas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_in = path_en + \"passo6/enfile\"\n",
      "\n",
      "for j in range(42):\n",
      "    f = codecs.open(data_in + str(j) + '.txt', encoding='utf-8')\n",
      "    f.seek(0)\n",
      "    text = f.read().split()\n",
      "\n",
      "    frase = []\n",
      "    for i in range(len(text)):\n",
      "        if text[i] != u'.':\n",
      "            frase.append(text[i])\n",
      "        else:\n",
      "            sentences.append(frase)\n",
      "            frase = []\n",
      "\n",
      "    if len(frase) != 0:\n",
      "        sentences.append(frase)\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, alpha=0.05, cbow_mean =1, workers=4)\n",
      "\n",
      "model.save(\"model-english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Para carregar os modelos depois de salvos:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_pt = gensim.models.Word2Vec.load(path_pt + \"model-portuguese\")\n",
      "model_en = gensim.models.Word2Vec.load(path_en + \"model-english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alguns testes com fun\u00e7\u00f5es"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print model_pt.most_similar(positive=['mulher', 'rei'], negative=['homem'], topn=1)\n",
      "print model_pt.doesnt_match(\"gato cachorro boi cadeira\".split())\n",
      "print model_pt.doesnt_match(\"gato cachorro boi pano\".split())\n",
      "print model_pt.similarity('mulher', 'homem')\n",
      "print model_pt.accuracy('questions-words-pt.txt')\n",
      "V = model_pt['computador']  # raw numpy vector of a word\n",
      "\n",
      "print model_en.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
      "print model_en.doesnt_match(\"cat dog bird chair\".split())\n",
      "print model_en.doesnt_match(\"cat dog computer bird\".split())\n",
      "print model_en.similarity('woman', 'man')\n",
      "print model_en.accuracy('questions-words-en.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Cria\u00e7\u00e3o do dicion\u00e1rio bil\u00edngue baseado na frenqu\u00eancia das palavras"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Para o Ingl\u00eas:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist_en = nltk.corpus.stopwords.words('english')\n",
      "for i in range(len(stoplist_en)):\n",
      "    stoplist_en[i] = stoplist_en[i].decode('utf-8')\n",
      "    \n",
      "    \n",
      "#5000 Mais frequentes\n",
      "\n",
      "cincomil_en = []\n",
      "lista = []\n",
      "repetidos_en = []\n",
      "\n",
      "for j in range(42):\n",
      "    data = path_en + \"passo6/fileen'\n",
      "    f = codecs.open(data + str(j) + '.txt', 'r','utf-8')\n",
      "    f.seek(0)\n",
      "    text = f.read().split()\n",
      "\n",
      "    sem_stop = []\n",
      "\n",
      "    for i in range(len(text)):\n",
      "        if text[i] not in stoplist_en and text[i] != '.':\n",
      "            sem_stop.append(text[i])\n",
      "        \n",
      "    cnt = Counter(sem_stop).most_common(3000)\n",
      "    \n",
      "\n",
      "    for k in range(3000):\n",
      "        if cnt[k][0] not in lista:\n",
      "            cincomil_en.append(cnt[k])\n",
      "            lista.append(cnt[k][0])\n",
      "        else:\n",
      "            repetidos_en.append(cnt[k])\n",
      "    f.close()\n",
      "print len(cincomil_en), cincomil_en[0]\n",
      "print len(repetidos_en)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_en = path_en + \"mais_freq.txt\"\n",
      "file2 = codecs.open(data_en, encoding='utf-8',mode='w')\n",
      "for i in range(len(cincomil_en)):\n",
      "    file2.write(cincomilen[i][0])\n",
      "    file2.write('\\n')\n",
      "file2.close() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Para o Portugu\u00eas:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist_pt = nltk.corpus.stopwords.words('portuguese')\n",
      "for i in range(len(stoplist_pt)):\n",
      "    stoplist_pt[i] = stoplist_pt[i].decode('utf-8')\n",
      "    \n",
      "    \n",
      "#5000 Mais frequentes\n",
      "\n",
      "cincomil_pt = []\n",
      "lista = []\n",
      "repetidos_pt = []\n",
      "\n",
      "for j in range(26):\n",
      "    data = path_pt + \"passo6/filept'\n",
      "    f = codecs.open(data + str(j) + '.txt', 'r','utf-8')\n",
      "    f.seek(0)\n",
      "    text = f.read().split()\n",
      "\n",
      "    sem_stop = []\n",
      "\n",
      "    for i in range(len(text)):\n",
      "        if text[i] not in stoplist_pt and text[i] != '.':\n",
      "            sem_stop.append(text[i])\n",
      "        \n",
      "    cnt = Counter(sem_stop).most_common(3000)\n",
      "    \n",
      "\n",
      "    for k in range(3000):\n",
      "        if cnt[k][0] not in lista:\n",
      "            cincomil_pt.append(cnt[k])\n",
      "            lista.append(cnt[k][0])\n",
      "        else:\n",
      "            repetidos_pt.append(cnt[k])\n",
      "    f.close()\n",
      "    \n",
      "print len(cincomil_pt), cincomil_pt[0]\n",
      "print len(repetidos_pt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_pt = path_pt + \"mais_freq.txt\"\n",
      "file = codecs.open(data_pt, encoding='utf-8',mode='w')\n",
      "for i in range(len(cincomil_pt)):\n",
      "    file.write(cincomil_pt[i][0])\n",
      "    file.write('\\n')\n",
      "file.close()    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cria\u00e7\u00e3o do dicion\u00e1rio bil\u00edngue nmanualmente usando as palavras do portugu\u00eas como base, e ficando da seguinte maneira:\n",
      "\u00e9 is\n",
      "g\u00eanero gender\n",
      "esp\u00e9cie specie"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}