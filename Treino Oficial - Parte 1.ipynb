{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "import codecs\n",
      "import unicodedata\n",
      "\n",
      "\n",
      "path_en = \"/home/lili/Documents/Projeto Final do Mestrado/treino/ingles/\"\n",
      "path_pt = \"/home/lili/Documents/Projeto Final do Mestrado/treino/portugues/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo a passo\n",
      "\n",
      "1. Reduzir n\u00famero de documentos\n",
      "2. Do grandearquivo retirar <doc>, </doc>\n",
      "3. Retirar http, www, </br> \n",
      "4. Script tokenizer\n",
      "5. Retirar pontua\u00e7\u00e3o (normal mais pontua\u00e7\u00e3o estranha), separar em frases e palavras em letras min\u00fasculas\n",
      "6. Substituir 2,3,4-gramas (colocar os gerados + os novos)\n",
      "7. Retirar stopwords "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo 1: Reduzir n\u00famero de documentos\n",
      "\n",
      "GRANDE ARQUIVOS - como s\u00e3o milhares de arquivos e cada pasta tem 100, aqui \u00e9 para pegar os 100 arquivos dentro de cada pasta para transformar num s\u00f3"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Ingl\u00eas\n",
      "\n",
      "data1 = path_en + \"wiki-en/*\"\n",
      "datasimpl = path_en + \"grandearquivo/grandearquivo\"\n",
      "pastas = glob.glob(data1)\n",
      "\n",
      "for i in range(len(pastas)):\n",
      "    file = open(datasimpl + str(i)+ '.txt',\"w\")\n",
      "    end_dentro_pasta = pastas[i] + '/*'\n",
      "    dentro_da_pasta = glob.glob(end_dentro_pasta)\n",
      "    for j in range(len(dentro_da_pasta)):\n",
      "        abrir = open(dentro_da_pasta[j])\n",
      "        abrir.seek(0)\n",
      "        arquivo = abrir.read()\n",
      "        file.write(arquivo)\n",
      "    file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Portugu\u00eas\n",
      "\n",
      "data1 = path_pt + \"wiki-pt/*\"\n",
      "datasimpl = path_pt + \"grandearquivo/grandearquivo\"\n",
      "pastas = glob.glob(data1)\n",
      "\n",
      "for i in range(len(pastas)):\n",
      "    file = open(datasimpl + str(i)+ '.txt',\"w\")\n",
      "    end_dentro_pasta = pastas[i] + '/*'\n",
      "    dentro_da_pasta = glob.glob(end_dentro_pasta)\n",
      "    for j in range(len(dentro_da_pasta)):\n",
      "        abrir = open(dentro_da_pasta[j])\n",
      "        abrir.seek(0)\n",
      "        arquivo = abrir.read()\n",
      "        file.write(arquivo)\n",
      "    file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo 2: Remo\u00e7\u00e3o das linhas com <doc>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Portugu\u00eas\n",
      "\n",
      "data_in = path_pt + \"grandearquivo/grandearquivo*\"\n",
      "data_out = path_pt + \"passo1/ptfile\"' \n",
      "\n",
      "files=glob.glob(data_in)   \n",
      "\n",
      "bad_words = ['<doc', '</doc>']\n",
      "\n",
      "for i in range(len(files)):\n",
      "    with codecs.open(data_in + str(i) + '.txt', encoding='utf-8') as oldfile, codecs.open(data_out + str(i)+ '.txt', encoding='utf-8', mode='w') as newfile:\n",
      "        for line in oldfile:\n",
      "            if not any(bad_word in line for bad_word in bad_words):\n",
      "                newfile.write(line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Ingl\u00eas\n",
      "\n",
      "data_in = path_en + \"grandearquivo/grandearquivo*\"\n",
      "data_out = path_en + \"passo1/enfile\"' \n",
      "\n",
      "files=glob.glob(data_in)   \n",
      "\n",
      "bad_words = ['<doc', '</doc>']\n",
      "\n",
      "for i in range(len(files)):\n",
      "    with codecs.open(data_in + str(i) + '.txt', encoding='utf-8') as oldfile, codecs.open(data_out + str(i)+ '.txt', encoding='utf-8', mode='w') as newfile:\n",
      "        for line in oldfile:\n",
      "            if not any(bad_word in line for bad_word in bad_words):\n",
      "                newfile.write(line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo 3: Remo\u00e7\u00e3o dos endere\u00e7os web (qualquer token que contenha http ou www )\n",
      "Portugu\u00eas tem 26 arquivos e o Ingl\u00eas tem 42, por isso os ranges tem esse tamanho"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Portugu\u00eas\n",
      "\n",
      "data_in = path_pt + \"passo1/ptfile\"\n",
      "data_out = path_pt + \"passo2/ptfile\"\n",
      "\n",
      "for i in range(26):\n",
      "    abrir = codecs.open(data_in + str(i) + '.txt', encoding='utf-8')\n",
      "    abrir.seek(0)\n",
      "    texto = abrir.read().split()\n",
      "    text = codecs.open(data_out + str(i) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "\n",
      "    for i in range(len(texto)):\n",
      "        if (u'www' not in texto[i]) and (u'http' not in texto[i]) and (u'</br>' not in texto[i]) and (u'.com' not in texto[i]):\n",
      "            text.write(texto[i])\n",
      "            text.write(' ')\n",
      "    text.close()\n",
      "abrir.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Ingl\u00eas\n",
      "\n",
      "data_in = path_en + \"passo1/enfile\"\n",
      "data_out = path_en + \"passo2/enfile\"\n",
      "\n",
      "for i in range(42):\n",
      "    abrir = codecs.open(data_in + str(i) + '.txt', encoding='utf-8')\n",
      "    abrir.seek(0)\n",
      "    texto = abrir.read().split()\n",
      "    text = codecs.open(data_out + str(i) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "\n",
      "    for i in range(len(texto)):\n",
      "        if (u'www' not in texto[i]) and (u'http' not in texto[i]) and (u'</br>' not in texto[i]) and (u'.com' not in texto[i]):\n",
      "            text.write(texto[i])\n",
      "            text.write(' ')\n",
      "    text.close()\n",
      "abrir.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo 4: Script tokenizer\n",
      "\n",
      "Arquivos salvos na pasta passo3"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Passo 5: Retirar pontua\u00e7\u00e3o (normal mais pontua\u00e7\u00e3o estranha) e guardar palavras em letras min\u00fasculas\n",
      "\n",
      "Nesse processo foram feitas duas c\u00f3pias para cada arquivo: uma sem pontua\u00e7\u00e3o nenhuma e a outra quase sem pontua\u00e7\u00e3o, restando apenas as de final de frase (\u2018.\u2019, \u2018 !\u2019 e \u2018 ?\u2019) que foram transformadas em ponto final."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Portugu\u00eas\n",
      "\n",
      "data_in = path_pt + \"passo3/ptfile\"\n",
      "data_out = path_pt + \"passo4/ptfile\"\n",
      "data_out2 = path_pt + \"novop4/ptfile\"\n",
      "\n",
      "pont =[u'!', u'\"', u'#', u'$', u'%', u'&', u\"'\", u'(', u')', u'*', u'+', u',', u'-', u'/', u':', u';', u'<', u'=', \n",
      "       u'>', u'?', u'@', u'[', u'\\\\', u']', u'^', u'_', u'`', u'{', u'|', u'}', u'~', u'\u2013', u'\u201d', u'\u201c', u'\u2192', u'\u2019',\n",
      "       u'..', u'...', u'\u00b0', u'\u2014', u'\u2022', u'\u2026', u'\u2212', u'\u2032', u'\u2033', u'\u00ab', u'\u00bb', u'\u2018', u'\u2020', u'\u00b4', u'\u2194']\n",
      "\n",
      "for j in range(26):\n",
      "    abrir = codecs.open(data_in + str(j) + '.txt', encoding = 'utf-8', mode = 'r')\n",
      "    \n",
      "    text1 = codecs.open(data_out + str(j) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "    text2 = codecs.open(data_out2 + str(j) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "    \n",
      "    abrir.seek(0)\n",
      "    t = abrir.read().split()\n",
      "    \n",
      "    for i in range(len(t)):\n",
      "        if t[i] not in pont:\n",
      "            text1.write(t[i].lower())\n",
      "            text1.write(' ')\n",
      "            \n",
      "            text2.write(t[i].lower())\n",
      "            text2.write(' ')\n",
      "            \n",
      "        else:\n",
      "            if t[i] == u'?' or t[i] == u'!':\n",
      "                text1.write(u'.')\n",
      "                text1.write(' ')\n",
      "abrir.close()\n",
      "text1.close()\n",
      "text2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Para o Ingl\u00eas\n",
      "\n",
      "data_in = path_en + \"passo3/enfile\"\n",
      "data_out = path_en + \"passo4/enfile\"\n",
      "data_out2 = path_en + \"novop4/enfile\"\n",
      "\n",
      "pont =[u'!', u'\"', u'#', u'$', u'%', u'&', u\"'\", u'(', u')', u'*', u'+', u',', u'-', u'/', u':', u';', u'<', u'=', \n",
      "       u'>', u'?', u'@', u'[', u'\\\\', u']', u'^', u'_', u'`', u'{', u'|', u'}', u'~', u'\u2013', u'\u201d', u'\u201c', u'\u2192', u'\u2019',\n",
      "       u'..', u'...', u'\u00b0', u'\u2014', u'\u2022', u'\u2026', u'\u2212', u'\u2032', u'\u2033', u'\u00ab', u'\u00bb', u'\u2018', u'\u2020', u'\u00b4', u'\u2194']\n",
      "\n",
      "for j in range(26):\n",
      "    abrir = codecs.open(data_in + str(j) + '.txt', encoding = 'utf-8', mode = 'r')\n",
      "    \n",
      "    text1 = codecs.open(data_out + str(j) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "    text2 = codecs.open(data_out2 + str(j) + '.txt', encoding = 'utf-8', mode = 'w')\n",
      "    \n",
      "    abrir.seek(0)\n",
      "    t = abrir.read().split()\n",
      "    \n",
      "    for i in range(len(t)):\n",
      "        if t[i] not in pont:\n",
      "            text1.write(t[i].lower())\n",
      "            text1.write(' ')\n",
      "            \n",
      "            text2.write(t[i].lower())\n",
      "            text2.write(' ')\n",
      "            \n",
      "        else:\n",
      "            if t[i] == u'?' or t[i] == u'!':\n",
      "                text1.write(u'.')\n",
      "                text1.write(' ')\n",
      "abrir.close()\n",
      "text1.close()\n",
      "text2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}